<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sourav Chakraborty</title>

  <meta name="author" content="Sourav Chakraborty">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cu-logo.png">
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@500&display=swap" rel="stylesheet">
  <style>
    p {
      text-align: justify;
    }
  </style>
  <script type="text/javascript"
    async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=PT+Sans&display=swap" rel="stylesheet">
 <!-- manual edit for gentium -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Gentium+Basic:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sourav Chakraborty</name><br>
                <span style="font-size: small; color: rgb(222, 144, 99);">Ph.D. Student · Theoretical Machine Learning · University of Colorado</span>
                <p style="text-align: center; font-style: italic; color: #555; font-size: small;">
                  "All models are wrong, but some are useful." – George Box
                </p>

                <p style="text-align: center; font-size: small; color: gray; margin-top: 5px;">
                  [Name pronounced either as <span style="color:#c22003"><em>Saw-rv</em></span> or <span style="color:#c22003"><em>Show-oo-rob</em></span> (Bengali)]
                </p>
              </p>
              
              <p>
                I am a Ph.D. student at the <a href="https://www.colorado.edu/">University of Colorado Boulder</a>, working as a Graduate Research Assistant at the <a href="https://www.colorado.edu/cs/">Department of Computer Science</a>. I am advised by <a href="https://spot.colorado.edu/~lich1539/">Prof. Lijun Chen</a>.
              </p>
              <p>
                My research focuses on the mathematical foundations of sequential decision-making under uncertainty, particularly, Multi-Armed Bandit models and Reinforcement Learning.
              </p>

              <!-- <p>
                Outside of research, teaching and mentorship are a crucial part of my academic work. I have served as a Teaching Assistant and the Instructor of Record for multiple semesters. I also served as the CS Department Lead Teaching Assistant from 2022 to 2025, where I coordinated instructional strategies and mentored the graduate teaching cohort across the department.

Prior to my doctoral studies, I completed my Master’s in Computer Science at CU Boulder. Before moving to the U.S., I spent time in the industry as a Software Engineer at Flipkart in Bangalore, India, designing scalable search pipelines and data recovery platforms. I earned my Bachelor's degree from BIT Mesra.
              </p> -->
              
              <p>
                Prior to my Ph.D., I completed a Master’s in Computer Science at the University of Colorado Boulder. I earned my bachelor's degree from the <a href="https://www.bitmesra.ac.in/">Birla Institute of Technology, Mesra</a> in Ranchi, India, and worked as a software engineer at <a href="https://www.linkedin.com/company/flipkart/">Flipkart</a> in Bangalore before moving to the U.S.
              </p>
              
              <p>
                Beyond research, I find myself drawn to <a href="https://letterboxd.com/souravchk/">cinema</a>, <a href="https://www.goodreads.com/user/show/50985172-sourav-chakraborty">literature</a>, and music. 
                Cricket has been a constant since childhood: I played for a local club at school and later for the Flipkart team, and now follow the game closely as a spectator.
              </p>

              <p>
                <span style="color: rgb(186, 69, 10)"><strong>Fun Fact.</strong></span> My <a href="https://en.wikipedia.org/wiki/Erd%C5%91s_number">Erdős Number</a> is 4 and <a href="https://www.csauthors.net/sourav-chakraborty-009/">Djikstra Number</a> is 5.
              </p>
              <br>
              <p style="text-align:center">
                <a href="mailto:sourav.chakraborty@colorado.edu">Email</a> &nbsp;&bull;&nbsp;
                <a href="https://www.linkedin.com/in/souravchk/">LinkedIn</a> &nbsp;&bull;&nbsp;
                <a href="">CV</a> &nbsp;&bull;&nbsp;
                <a href="https://scholar.google.com/citations?hl=en&user=19m6XE4AAAAJ&view_op=list_works&gmla=AGd7smHpS_EnoiQAq2ZKg0HR-LbWuXLjTYG_sAkcVx5h64AYMayt6U69y1LvGuhMHYBrj_l-Fm_KIMJX0olMTkBC">Google Scholar</a> <br>
                <span style="display:inline-block; margin-top:5px;">
                  <a href="https://letterboxd.com/souravchk/">Letterboxd</a> &nbsp;&bull;&nbsp;
                  <a href="https://www.goodreads.com/user/show/50985172-sourav-chakraborty">Goodreads</a>
                </span>
              </p>
              
              </p>
            </td>
            <td style="padding:0.5%; width:75%; max-width:75%;">
              <a href="images/ouray-full.jpg" target="_blank">
                <img 
                  style="width:100%; max-width:100%; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin-top: 70px;" 
                  alt="profile photo" 
                  src="images/ouray-cut.jpg" 
                  class="hoverZoomLink">
              </a>
              <p style="text-align: center; font-style: italic; font-size: x-small; color: #666; margin-top: 1px;">
                Taken in Ouray, Colorado, <br> known as "The Switzerland of America".
              </p>
            </td>
              <!-- <a href="images/ouray-brown-full.jpg" target="_blank">
                <img 
                  style="width:100%; max-width:100%; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin-top: 95px;" 
                  alt="profile photo" 
                  src="images/ouray-brown.jpg" 
                  class="hoverZoomLink">
              </a>
              <p style="text-align: center; font-style: italic; font-size: x-small; color: #666; margin-top: 1px;">
                Taken in Ouray, Colorado, <br> known as "The Switzerland of America".
              </p>
            </td> -->
            
            
            
            
            
          </tr>

        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <heading>Updates & News</heading>
              <ul>
                <!-- <li><strong>2026-02</strong>: <span style="color: green">Defended</span> <strong>PhD Proposal</strong> (Comprehensive) Exam! -->
                <li><strong>2026-01</strong>: Paper <span style="color:green">accepted</span> at <strong> AISTATS 2026!</strong> Tangier, Morocco.</li>
                <li><strong>2026-01</strong>: <strong>Two</strong> papers <span style="color:green">accepted</span> at <strong> L4DC 2026!</strong>, Los Angeles, CA.</li>
                <li><strong>2025-09</strong>: Paper <span style="color:green">accepted</span> at <strong> NeurIPS 2025 workshop ARLET!</strong>, San Diego, CA.</li>
                <li><strong>2025-07</strong>: <a href="https://arxiv.org/pdf/2508.19466">Paper</a> <span style="color:green">accepted</span> at <strong>IEEE CDC 2025!</strong>, Rio De Janeiro, Brazil.</li>
                <li><strong>2024-12</strong>: <span style="color: green">Cleared</span> the Computer Science <strong>PhD Prelims</strong> (Area) Exam!</li>
                <li><strong>2024-03</strong>: Continuing as a <a href="https://www.colorado.edu/cs/">CS</a> <strong>Department Lead TA</strong> for the AY 2024-25.</li>
                <li><strong>2024-01</strong>: <a href="https://arxiv.org/pdf/2403.10819">My first paper</a> got <span style="color:green">accepted</span> at <strong> IEEE ACC 2024!</strong>, Toronto, Canada.</li>
                <li><strong>2023-08</strong>: Guest lectured for Fall 2023 Advanced ML course on Bandit models.</li>
		<li><strong>2023-03</strong>: <span style="color: green">Elected</span> as the student representive for the <a href="https://bouldercsgrads.org/officers">CS Graduate Committee</a>.</li>
		<li><strong>2023-03</strong>: Re-appointed as a <a href="https://www.colorado.edu/cs/">CS</a> <strong>Department Lead TA</strong> for the AY 2023-24.</li>
                <li><strong>2022-08</strong>: <strong>Started my PhD</strong> journey at the University of Colorado Boulder in CS!</li>
                <li><strong>2022-05</strong>: <span style="color: green">Graduated</span> with <strong>Master of Science (MS)</strong> in Computer Science!
                <li><strong>2022-04</strong>: <span style="color:green">Successfully defended</span> <strong>Master's thesis!</strong> (<a href="https://www.proquest.com/pqdtglobal/docview/2681066075/AD79799B18734CCAPQ/1?accountid=14503">link</a>/
                  <a href="https://drive.google.com/file/d/1wh9OgLBMrK8iCFypy_4BC1VmjSxOK93M/view?usp=sharing">slides</a>)</li>
                <li><strong>2022-02</strong>: Got offer for Ph.D from <a href="https://www.colorado.edu/cs/">CS @ Colorado</a> for Fall 2022!</li>
                <li><strong>2019-08</strong>: Started Master of Science in Computer Science at <a href="https://www.colorado.edu/">Colorado!</a></li>
                <li><strong>2016-12</strong>: Joined <a href="https://www.linkedin.com/company/flipkart/">Flipkart</a> as a Software Engineer in Bangalore, India. </li>
		        <li><strong>2016-06</strong>: Bachelor's in engineering completed from <a href="https://www.bitmesra.ac.in/">BIT Mesra</a>.</li>
              </ul>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <heading>Awards & Honors</heading>
             
              <ul class="news">
                <!-- <li><strong>2025-08</strong>: Recepient of the  departmental <strong><span style="color: darkmagenta;">Research Assistant Fellowship</span></strong> for AY 2025-26.</li> -->
                <li><strong>2026-01</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Conference Travel Award </span></strong> for CDC in Rio de Janeiro, Brazil 2025.</li>
                <li><strong>2025-11</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Graduate Research Assistant Fellowship</span></strong> for Spring 2026.</li>
                <li><strong>2025-04</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Outstanding Teaching Assistant Award</span></strong> from the CS department.</li>
                <li><strong>2025-04</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Outstanding Service Award</span></strong> from the CS department.</li>
		            <li><strong>2024-04</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Outstanding Research Paper Award</span></strong> from the CS department.</li>
                <li><strong>2024-04</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Full Conference Travel Fellowship </span></strong> for ACC in Toronto, Canada 2024.</li>
                <li><strong>2024-04</strong>: Recepient of the  <strong>CU research Expo</strong> research <strong><span style="color: darkmagenta;">poster award</span></strong> for the annual year 2023-24.</li>
                <li><strong>2024-03</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Publication Recognition Award</span></strong> for the annual year 2023-24.</li>
                <li><strong>2022-09</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Early Career Development Fellowship </span></strong> from the CS department.</li>
                <li style="padding-left:7ch; text-indent:-7ch;"><strong>2022-05</strong>: Recepient of the  <strong><span style="color: darkmagenta;">Lloyd Botway Award for Outstanding Master's student </span></strong> for "recognizing <br> excellence in academics, teaching, research, and service <strong>among the graduating cohort.</strong>"</li>
                <li><strong>2022-04</strong>: Recepient of the  <strong>CU research Expo</strong> research <strong><span style="color: darkmagenta;">poster award</span></strong> for the annual year 2021-22.</li>
                <li><strong>2022-04</strong>: Selected as a <strong><span style="color: darkmagenta;">Lead Teaching Assistant</span></strong> (department lead) for <a href="https://www.colorado.edu/cs/">CS @ CU</a> for the annual year.</li>
              </ul>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I investigate how structural, physical, and strategic constraints shape the limits of learnability in uncertain environments. My work focuses on three core pillars:
                  </p>
                  <ul style="margin-top:8px; margin-bottom:0px; padding-left:20px;">
                    <li><strong>Graph-Constrained Bandits.</strong> Characterizing the anatomy of regret when actions are restricted to local moves on evolving graphs, and designing algorithms that balance statistical learning with physical navigation costs.</li>
                    <li><strong>Incentivized Exploration.</strong> Creating mechanisms that compensate myopic agents to explore under non-stationary rewards and biased feedback, connecting online learning with incentive design.</li>
                    <li><strong>Scalable Multi-Agent Systems.</strong> Exploiting policy-dependent locality to overcome the curse of dimensionality in decentralized MARL, using spectral conditions to enable provably sound, communication-free learning at scale.</li>
                  </ul>
                </td>
                

              </tr>
            </tbody>
          </table>
          

       
        
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            
            <tr onmouseout="aistats_stop()" onmouseover="aistats_start()">
              <td style="padding:0px;width:25%;vertical-align:top; text-align:center; padding-left:12px;">
                <div class="one">
                  <div class="two" id='aistats_image'>
                    <img src='images/aistats-logo.webp' width="176"></div>
                  <img src='images/aistats-logo.webp' width="176">
                </div>
                <script type="text/javascript">
                  function aistats_start() {
                    document.getElementById('aistats_image').style.opacity = "1";
                  }

                  function aistats_stop() {
                    document.getElementById('aistats_image').style.opacity = "0";
                  }
                  aistats_start()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="">
                  <papertitle> Multi-Agent Lipschitz Bandits.</papertitle>
                </a>
                <br>
                <strong>Sourav Chakraborty*</strong>, Amit Kiran Rege*, Claire Monteleoni, Lijun Chen  
                <br>
                <em><strong><span style="color: green">Accepted</span> at International Conference of Artificial Intelligence and Statistics (AISTATS) 2026</strong> in Tangier, Morocco.</em>
                <br>
                <!-- <a href="">IEEE version </a>&nbsp/ -->
                <!-- <a href="">arXiv</a> -->
                <p></p>
                <div style="font-size: small;text-align: justify; margin-top: 6px;">
                  <!-- Abstract removed for brevity.
                  We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon \(T\). We propose a modular protocol that first solves the multi-agent coordination problem, i.e., identifying and seating players on distinct, high-value regions via a novel maxima-directed search, and then decouples the problem into \(N\) independent single-player Lipschitz bandits. We establish a near-optimal regret bound of \( \tilde{O}(T^{(d+1)/(d+2)}) \) plus a \(T\), independent coordination cost, matching the single-player rate. Our framework is, to our knowledge, the first to provide such guarantees and extends to general distance-threshold collision models.
                  -->
                </div>
                  
              </td>
            </tr>

            <tr onmouseout="l4dc1_stop()" onmouseover="l4dc1_start()">
              <td style="padding:0px;width:25%;vertical-align:top; text-align:right;">
                <div class="one">
                  <div class="two" id='l4dc1_image'>
                    <img src='images/l4dc2026.jpg' width="90"></div>
                  <img src='images/l4dc2026.jpg' width="90">
                </div>
                <script type="text/javascript">
                  function l4dc1_start() {
                    document.getElementById('l4dc1_image').style.opacity = "1";
                  }

                  function l4dc1_stop() {
                    document.getElementById('l4dc1_image').style.opacity = "0";
                  }
                  l4dc1_start()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="">
                  <papertitle> Flickering Multi-Armed Bandits.</papertitle>
                </a>
                <br>
                <strong>Sourav Chakraborty*</strong>, Amit Kiran Rege*, Claire Monteleoni, Lijun Chen  
                <br>
                <em><strong><span style="color: green">Accepted</span> at Learning for Dynamics & Control Conference (L4DC) 2026</strong> in Los Angeles, California, United States. </em> <strong style="color:#f12e0b;">(Selected as an Oral Presentation)</strong>
                <br>
                <!-- <a href="">IEEE version </a>&nbsp/ -->
                <!-- <a href="">arXiv</a>  -->
                <p></p>
                <div style="font-size: small;text-align: justify; margin-top: 6px;">
                  <!-- Abstract removed for brevity.
                  We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erd\H{o}s--R\'enyi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.
                  -->
                </div>
                  
              </td>
            </tr>

            <tr onmouseout="l4dc2_stop()" onmouseover="l4dc2_start()">
              <td style="padding:0px;width:25%;vertical-align:top; text-align:right;">
                <div class="one">
                  <div class="two" id='l4dc2_image'>
                    <img src='images/l4dc2026.jpg' width="90"></div>
                  <img src='images/l4dc2026.jpg' width="90">
                </div>
                <script type="text/javascript">
                  function l4dc2_start() {
                    document.getElementById('l4dc2_image').style.opacity = "1";
                  }

                  function l4dc2_stop() {
                    document.getElementById('l4dc2_image').style.opacity = "0";
                  }
                  l4dc2_start()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="">
                  <papertitle> A Unified Framework for Locality in Scalable MARL. </papertitle>
                </a>
                <br>
                <strong>Sourav Chakraborty*</strong>, Amit Kiran Rege*, Claire Monteleoni, Lijun Chen  
                <br>
                <em><strong><span style="color: green">Accepted</span> at Learning for Dynamics & Control Conference (L4DC) 2026</strong> in Los Angeles, California, United States. </em>
                
                <br>
                <!-- <a href="">IEEE version </a>&nbsp/ -->
                <!-- <a href="">arXiv</a> -->
                <p></p>
                <div style="font-size: small;text-align: justify; margin-top: 6px;">
                  <!-- Abstract removed for brevity.
                  Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a <em>policy-dependent</em> phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, \(H^\pi\), which decouples the environment's sensitivity to state (\(E^{\mathrm{s}}\)) and action (\(E^{\mathrm{a}}\)) from the policy's sensitivity to state (\(\Pi(\pi)\)). This decomposition reveals that locality can be induced by a smooth policy (small \(\Pi(\pi)\)) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition \( \rho(E^{\mathrm{s}}+E^{\mathrm{a}}\Pi(\pi)) < 1 \) for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.
                  -->
                </div>
                  
              </td>
            </tr>

            <tr onmouseout="arlet_stop()" onmouseover="arlet_start()">
              <td style="padding:0px;width:25%;vertical-align:top;text-align:right;  padding-left:20px;" >
                <div class="one">
                  <div class="two" id='cdc25_image'>
                    <img src='images/neurips_logo.png' width="154"></div>
                  <img src='images/neurips_logo.png' width="154">
                </div>
                <script type="text/javascript">
                  function arlet_start() {
                    document.getElementById('cdc25_image').style.opacity = "1";
                  }
  
                  function arlet_stop() {
                    document.getElementById('cdc25_image').style.opacity = "0";
                  }
                  arlet_start()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="">
                  <papertitle> Bandit Learning on Dynamic Graphs.</papertitle>
                </a>
                <br>
                <strong>Sourav Chakraborty*</strong>, Amit Kiran Rege*, Claire Monteleoni, Lijun Chen  
                <br>
                <em><strong><span style="color: green">Accepted</span> at ARLET Workshop at Neural Information Processing Systems (NeurIPS) 2025</strong> in San Diego, California, United States.</em>
                <br>
                <!-- <a href="">IEEE version </a>&nbsp/ -->
                <a href="">arXiv</a>
                <p></p>
                <div style="font-size: small;text-align: justify; margin-top: 6px;">
                  <!-- Abstract removed for brevity.
                  We study an online learning setting where an agent's actions are constrained to local movements on a dynamic graph, a setting that captures scenarios such as autonomous reconnaissance. This problem highlights a core challenge in adaptive systems: how to learn effectively with only partial, localized feedback in a non-stationary environment. We propose a set of structural conditions, termed <em>Recurrent Reachability</em> and <em>Temporal Stability</em>, that are sufficient for learnability. Our analysis reveals a foundational <em>anatomy of regret</em>, decomposing it into a statistical learning cost and a physical navigation cost. We introduce a family of local algorithms, progressing from a canonical protocol to a more practical, adaptive variant, and culminating in a reward-aware exploration policy that achieves provably near-optimal regret on any graph sequence satisfying our conditions. We corroborate our theory in a disaster-response simulation.
                  -->
                </div>
                  
              </td>
            </tr>

    

            <tr onmouseout="cdc25_stop()" onmouseover="cdc25_start()">
              <td style="padding:0px;width:25%;vertical-align:top;">
                <div class="one">
                  <div class="two" id='cdc25_image'>
                    <img src='images/cdc25-intro-fig.png' width="220"></div>
                  <img src='images/cdc25-intro-fig.png' width="220">
                </div>
                <script type="text/javascript">
                  function cdc25_start() {
                    document.getElementById('cdc25_image').style.opacity = "1";
                  }
  
                  function cdc25_stop() {
                    document.getElementById('cdc25_image').style.opacity = "0";
                  }
                  cdc25_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                <a href="">
                  <papertitle> Incentivized Lipschitz Bandits.</papertitle>
                </a>
                <br>
                <strong>Sourav Chakraborty*</strong>, Amit Kiran Rege*, Claire Monteleoni, Lijun Chen  
                <br>
                <em><strong><span style="color: green">Accepted</span> at IEEE Conference on Decision and Control (CDC) 2025</strong> in Rio de Janeiro, Brazil.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/document/11312562">IEEE</a>&nbsp/
                <a href="https://arxiv.org/pdf/2508.19466">arXiv</a>
                <p></p>
                <div style="font-size: small;text-align: justify; margin-top: 6px;">
                  <!-- Abstract removed for brevity.
                  We study incentivized exploration in bandit settings with infinitely many arms over continuous metric spaces. A principal offers compensation to myopic agents to encourage exploration, but must contend with reward drift due to biased feedback. 

                  We develop discretization-based algorithms that achieve both sublinear regret and sublinear compensation, with guarantees of \( \widetilde{O}(T^{(d+1/d+2)}) \) where \( d \) is the covering dimension. We extend our approach to contextual bandits and validate it through simulations.
                  -->
                </div>
                  
              </td>
            </tr>

          <tr onmouseout="acc24_stop()" onmouseover="acc24_start()">
            <td style="padding:10px;width:25%;vertical-align:top; padding-left:20px">
              <div class="one">
                <div class="two" id='acc24_image'>
                  <img src='images/acc24-2.png' width="180"></div>
                <img src='images/acc24-2.png' width="180">
              </div>
              <script type="text/javascript">
                function acc24_start() {
                  document.getElementById('acc24_image').style.opacity = "1";
                }

                function acc24_stop() {
                  document.getElementById('acc24_image').style.opacity = "0";
                }
                acc24_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10644966">
                <papertitle> Incentivized Exploration in Non-stationary Stochastic Bandits.</papertitle>
              </a>
              <br>
              <strong>Sourav Chakraborty</strong> and Lijun Chen.
              <br>
              <em><strong><span style="color: green">Accepted</span> at IEEE American Control Conference (ACC) 2024</strong> in Toronto, Canada.</em>
              <br>
              <em><strong>Master's Thesis</strong>, <span style="font-size: small;">Committee: <a href="https://spot.colorado.edu/~lich1539/">Lijun Chen</a>,
                <a href="https://home.cs.colorado.edu/~raf/">Raf Frongillo</a>, <a href="https://www.bowaggoner.com/">Bo Waggoner.</a></span></em> 
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10644966">IEEE version</a>&nbsp/
              <a href="https://arxiv.org/pdf/2403.10819">arXiv</a>&nbsp/
              <a href="https://www.proquest.com/pqdtglobal/docview/2681066075/AD79799B18734CCAPQ/1?accountid=14503">thesis</a>
              <p></p>
              <div style="font-size: small;text-align: justify; margin-top: 6px;">
                <!-- Abstract removed for brevity.
                We study incentivized exploration in non-stationary bandit settings, where agents receive compensation to explore beyond their greedy choices, but report biased feedback. 

                We propose algorithms for both abruptly-changing and continuously-drifting environments, and show they achieve sublinear regret and compensation, even under biased rewards, thus enabling effective exploration over time.
                -->
              </div>
             <!-- <p>We explore incentivized exploration in the multiarmed bandit (MAB) problem with changing reward distributions and biased feedback. Our algorithms address abruptly and continuously changing environments, achieving sublinear regret and compensation, effectively incentivizing exploration despite challenges. -->
               </p>
              </td>
          </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <!-- <td style="padding:2.5%;width:100%;vertical-align:middle">
              <heading>Essays</heading>
              <ul>
                <li>[May 2018] - Exploring Probabilistic Data Structures: Bloom Filters <a href="https://opensourceforu.com/2018/05/exploring-probabilistic-data-structures-bloom-filters/">[link]</a></li>
              </ul>
            </td> -->
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:10%;vertical-align:middle;text-align: justify"><img width="100",height="100", src="images/cu-icon.png"></td>
            <td width="75%" valign="center">
              <ul>
                <li><strong>Fall 2025</strong>: Teaching Assistant for CSCI 2275 - Programming and Data Structures.</li>
                <li><strong>Summer 2025</strong>: Teaching Assistant for CSCI 1300 - Starting Computing</li>
                <li><strong>Spring 2025</strong>: Teaching Assistant for CSCI 3104 - Algorithms</li>   
                <li><strong>Fall 2024</strong>: Teaching Assistant for CSCI 3104 - Algorithms</li>   
                <li><strong>Summer 2024</strong>: Teaching Assistant for CSCI 2270 - Data Structures.</li>
                <li><strong>Spring 2024</strong>: Teaching Assistant for CSCI 3104 - Algorithms</li>
                <li><strong>Fall 2023</strong>: Teaching Assistant for CSCI 2275 - Programming and Data Structures.</li>
                <li><strong>Spring 2023</strong>: Teaching Assistant for CSCI 2270 - Data Structures.</li>
                <li><strong>Fall 2022</strong>: Teaching Assistant for CSCI 2270 - Data Structures.</li>
                <li><strong>Spring 2022</strong>: Teaching Assistant for CSCI 2270 - Data Structures.</li>
                <li><strong>Fall 2021</strong>: <strong><span style="color: red">Instructor</span></strong> for CSCI 1200 - Introduction to Computational Thinking.</li>
                <li><strong>Summer 2021</strong>: Teaching Assistant for CSCI 1300 - Starting Computing.</li>
                <li><strong>Spring 2021</strong>: Teaching Assistant for CSCI 1300 - Starting Computing.</li>
                <li><strong>Fall 2020</strong>: Teaching Assistant for CSCI 1300 - Starting Computing.</li>
                <li><strong>Summer 2020</strong>: <strong><span style="color: red">Instructor</span></strong> for CSCI 3022 - Introduction to Data Science with Probability & Statistics.</li>
            </ul>            
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:5px;width:10%;vertical-align:middle;text-align: justify"><img width="130",height="130", src="images/fk-logo.png"></td>
            <td width="75%" valign="center">
              <ul>
                <li>Built an end-to-end <strong>Searches and Shopping Ideas</strong> pipeline in Java Cascading that suggests contextually relevant queries, broadening what shoppers can discover as they type.  </li>
                <li>Designed <strong>learning-driven ranking signals</strong> that improved how relevant results surface across the search experience.</li>
                <li>Built a <strong>pluggable backup platform</strong> with MySQL drivers that matured into BRaaS (Backup Recovery as a Service), the centralized service safeguarding and restoring data for every Flipkart application.</li>
            </ul>            
            </td>
          </tr>
        </tbody></table>
  
  



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Personal Projects</heading>
            <p style="font-size:small">*Alphabetically</p>
          </td>
        </tr>
      </tbody></table>


      
      

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>

        <tr onmouseout="dmu_stop()" onmouseover="dmu_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='dmu_image'>
                <img src='images/dmu-results.png' width="200"></div>
              <img src='images/gridworld_mdp.png' width="150">
            </div>
            <script type="text/javascript">
              function dmu_start() {
                document.getElementById('dmu_image').style.opacity = "1";
              }

              function dmu_stop() {
                document.getElementById('dmu_image').style.opacity = "0";
              }
              dmu_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/tuhina2313/DMU_Project">
              <papertitle> Inverse Reinforcement Learning via Maximum Entropy Formulation.</papertitle>
            </a>
            <br>
            Tuhina Tripathi, Alexa Reed, <strong>Sourav Chakraborty</strong> 
            <br>
            <em>April </em>, 2022  
            <br>
            <a href="files/DMU_Project_Report.pdf">report</a> &nbsp/&nbsp
            <a href="https://github.com/tuhina2313/DMU_Project">code</a>&nbsp/&nbsp
            <a href="https://drive.google.com/file/d/1bngSjiP8bu8pldSw_C_z4oDS2hn0NkqM/view?usp=sharing">demo</a>&nbsp/&nbsp
            <a href="https://github.com/souravchk25/dmu-interface/tree/master/">interface-code</a>
            <p></p>
            <p>
              Final project for ASEN 5519: Decision Making Under Uncertainty. This project explores the use of Inverse Reinforcement Learning, via Maximum Entropy Formulation, in a Markov Decision Process. The concepts explored in this project were  demonstrated using a grid world environment. 
              </p> 
            </td>
        </tr>

        <tr onmouseout="imab_stop()" onmouseover="imab_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='imab_image'>
                <img src='images/imab_after.png' width="160"></div>
              <img src='images/imab_before.png' width="160">
            </div>
            <script type="text/javascript">
              function imab_start() {
                document.getElementById('imab_image').style.opacity = "1";
              }

              function imab_stop() {
                document.getElementById('imab_image').style.opacity = "0";
              }
              imab_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/souravchk25/multiarmed-bandits-reward-drift">
              <papertitle> Incentivized Exploration for Multi-Armed Bandits under Reward Drift.</papertitle>
            </a>
            <br>
            <strong>Sourav Chakraborty</strong> 
            <br>
            <em>September</em>, 2020  
            <br>
            <a href="https://arxiv.org/abs/1911.05142">original paper</a> &nbsp/&nbsp
            <a href="https://github.com/souravchk25/multiarmed-bandits-reward-drift">code</a>
            <p></p>
            <p>
              Just playing around with the <a href="https://arxiv.org/abs/1911.05142">paper by Liu & Wang et al</a>on Incentivized Exploration for Multi-Armed Bandits under Reward Drift
              where the players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on reward drift.
             </p> 
            </td>
        </tr> 

        <tr onmouseout="we_stop()" onmouseover="we_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='we_image'>
                <img src='images/swe_after.PNG' width="160"></div>
              <img src='images/swe_before.png' width="160">
            </div>
            <script type="text/javascript">
              function we_start() {
                document.getElementById('we_image').style.opacity = "1";
              }

              function we_stop() {
                document.getElementById('we_image').style.opacity = "0";
              }
              we_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://drive.google.com/file/d/1uWGw--LCC-tMqtquTySHe9aCjUHyccRK/view">
              <papertitle> Contextual vectorized representation of words: Soam word embeddings</papertitle>
            </a>
            <br>
            *<a href="https://www.amitbaranroy.com/">Amit Baran Roy</a>, <strong>Sourav Chakraborty</strong> 
            <br>
            <em>May</em>, 2020  
            <br>
            <a href="https://drive.google.com/file/d/1uWGw--LCC-tMqtquTySHe9aCjUHyccRK/view">report</a> &nbsp/&nbsp
            <a href="https://github.com/AmitProspeed/SOAM-Word-Embeddings">code</a>
            <p></p>
            <p>A word embedding model implementation based on the popular skipgram architecture. It involves alterations of the scoring algorithm to give more weightage to the context words that are closer to the target word in a skipgram sliding window.</p>
          </td>
        </tr> 


        <tr onmouseout="agt_stop()" onmouseover="agt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='agt_image'>
                <img src='images/agt_after.png' width="160"></div>
              <img src='images/agt_before.png' width="160">
            </div>
            <script type="text/javascript">
              function agt_start() {
                document.getElementById('agt_image').style.opacity = "1";
              }

              function agt_stop() {
                document.getElementById('agt_image').style.opacity = "0";
              }
              agt_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/souravchk25/alg-game-theory-project/blob/master/AGTFinalProjectReport_Sourav_Naga.pdf">
              <papertitle>Solving Games using the combination of Q-learning and Regret Matching Methods</papertitle>
            </a>
            <br>
            <strong>Sourav Chakraborty</strong>, <a href="https://www.linkedin.com/in/nagarajan-shanmuganathan">Nagarajan Shanmuganathan</a> 
            <br>
            <em>May</em>, 2020  
            <br>
            <a href="https://github.com/souravchk25/alg-game-theory-project/blob/master/AGTFinalProjectReport_Sourav_Naga.pdf">report</a> &nbsp/&nbsp
            <a href="https://github.com/souravchk25/alg-game-theory-project">code</a> 
            <p></p>
            <p>It is known well that Counterfactual regret minimization
              (CFR) has been used in games which have both terminal states and perfect recall to minimize regret. This project aims to relax those constraints
              and use a local no-regret algorithm (LONR) by <a href="https://arxiv.org/pdf/1910.03094.pdf">Kash et al</a>, which internally uses a Q-learning like update rule to games which do not have terminal states or
              perfect recall.</p>
          </td>
        </tr> 

        <tr onmouseout="thresh_stop()" onmouseover="thresh_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='thresh_image'>
                <img src='images/onw_after.PNG' width="160"></div>
              <img src='images/onw_before.PNG' width="160">
            </div>
            <script type="text/javascript">
              function thresh_start() {
                document.getElementById('thresh_image').style.opacity = "1";
              }

              function thresh_stop() {
                document.getElementById('thresh_image').style.opacity = "0";
              }
              thresh_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://drive.google.com/file/d/1euDcbe5lZ99nM89ukua4Pgpr2IvIAizB/view">
              <papertitle>Occupancy Network based 3D Image Reconstruction using Single-Depth View</papertitle>
            </a>
            <br>
            *<a href="https://www.amitbaranroy.com/">Amit Baran Roy</a>, <a href="https://www.linkedin.com/in/aparajitasingh10/">Aparajita Singh</a>, <strong>Sourav Chakraborty</strong>, <a href="https://www.linkedin.com/in/tanmai-gajula">Tanmai Gajula</a> 
            <br>
            <em>Feb-April</em>, 2020
            <br>
            <a href="https://drive.google.com/file/d/1euDcbe5lZ99nM89ukua4Pgpr2IvIAizB/view">report</a> &nbsp/&nbsp
            <a href="https://github.com/AmitProspeed/csci5922project">code</a>
            <br>
            <p></p>
            <p>
            The complete 3D geometry of an object from a single 2.5D depth view was acquired by using deep learning techniques such as generative adversarial networks and 3D convolution neural networks. The resolution of the final 3D voxelized output was improved by transforming the voxel representation into another representation called occupancy networks.
            </p>
          </td>
        </tr>
        </tbody></table>

	<p style="text-align:center;font-size:x-small;">
            Last updated: Jan 2026 <br> Thanks <a href="https://jonbarron.info/" style="font-size:x-small">Jon Barron!</a>
        </p>
	<div style="text-align: center; margin-top: 30px;">
  		<div style="transform: scale(1.0); transform-origin: top center;">
		     <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=bNSuFvzPxUteENuG2yIWqZnlr1-7hFjVir4LoDc1i94&co=bbdef7&cmo=3acc3a&cmn=ff5353&ct=020000'></script>
		</div>
	</div>
       
</body>

</html>
